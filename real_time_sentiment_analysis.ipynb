{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "criminal-fifth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "coastal-transsexual",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "raised-wonder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Kafka'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imported-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "everyday-concrete",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "muslim-subscription",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import pyspark.sql.types as TP\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row\n",
    "import sys\n",
    "from pyspark.streaming.kafka import KafkaUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "worldwide-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"kafka_tweet1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "actual-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "powered-absolute",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_schema = TP.StructType([\n",
    "  TP.StructField(name= 'id',          dataType= TP.StringType(),  nullable= True),\n",
    "  TP.StructField(name= 'label',       dataType= TP.IntegerType(),  nullable= True),\n",
    "  TP.StructField(name= 'tweet',       dataType= TP.StringType(),   nullable= True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ruled-ceiling",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data= spark.read.csv('D:/PySpark/PySpark-master/spark_streaming/datasets/twitter_sentiments.csv', schema= my_schema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sunrise-alpha",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+\n",
      "| id|label|               tweet|\n",
      "+---+-----+--------------------+\n",
      "|  1|    0| @user when a fat...|\n",
      "|  2|    0|@user @user thank...|\n",
      "|  3|    0|  bihday your maj...|\n",
      "|  4|    0|#model   i love u...|\n",
      "|  5|    0| factsguide: soci...|\n",
      "+---+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "motivated-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_1 = RegexTokenizer(inputCol= 'tweet' , outputCol= 'tokens', pattern= '\\\\W')\n",
    "\n",
    "stage_2 = StopWordsRemover(inputCol= 'tokens', outputCol= 'filtered_words')\n",
    "\n",
    "stage_3 = Word2Vec(inputCol= 'filtered_words', outputCol= 'vector', vectorSize= 100)\n",
    "\n",
    "model = LogisticRegression(featuresCol= 'vector', labelCol= 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "confidential-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline= Pipeline(stages=[stage_1, stage_2, stage_3, model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "commercial-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit = pipeline.fit(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "rotary-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(tweet_text):\n",
    "    try:\n",
    "    # filter the tweets whose length is greater than 0\n",
    "        #tweet_text = tweet_text.filter(lambda x: len(x) > 0)\n",
    "    # create a dataframe with column name 'tweet' and each row will contain the tweet\n",
    "        #rowRdd = tweet_text.map(lambda w: Row(tweet=w))\n",
    "    # create a spark dataframe\n",
    "        #wordsDataFrame = spark.createDataFrame(rowRdd)\n",
    "    # transform the data using the pipeline and get the predicted sentiment\n",
    "        pipelineFit.transform(wordsDataFrame).select('tweet','prediction').show()\n",
    "    except : \n",
    "        print('No data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "meaningful-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext( sc, batchDuration= 3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-secondary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eba06a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readMyRddsFromKafkaStream( readRdd ):\n",
    "  # Put RDD into a Dataframe\n",
    "    df = spark.read.json( readRdd )\n",
    "    df.registerTempTable( \"temporary_table\" )\n",
    "    df = spark.sql( \"\"\"SELECT tweet FROM temporary_table \"\"\" )\n",
    "    pipelineFit.transform(df).select('tweet','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "brutal-disclosure",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o214.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"C:\\BigData\\SPARK_2.4\\python\\pyspark\\sql\\utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"C:\\BigData\\SPARK_2.4\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o21.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`tweet`' given input columns: []; line 1 pos 7;\n'Project ['tweet]\n+- SubqueryAlias `temporary_table`\n   +- LogicalRDD false\n\r\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.immutable.List.map(List.scala:296)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:643)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\BigData\\SPARK_2.4\\python\\pyspark\\streaming\\util.py\", line 68, in call\n    r = self.func(t, *rdds)\n  File \"C:\\BigData\\SPARK_2.4\\python\\pyspark\\streaming\\dstream.py\", line 161, in <lambda>\n    func = lambda t, rdd: old_func(rdd)\n  File \"<ipython-input-18-64474ac8612d>\", line 7, in <lambda>\n    message= data.foreachRDD( lambda yourRdd: readMyRddsFromKafkaStream( yourRdd ) )\n  File \"<ipython-input-17-2bf111efdf4d>\", line 5, in readMyRddsFromKafkaStream\n    df = spark.sql( \"\"\"SELECT tweet FROM temporary_table \"\"\" )\n  File \"C:\\BigData\\SPARK_2.4\\python\\pyspark\\sql\\session.py\", line 767, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n  File \"C:\\BigData\\SPARK_2.4\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"C:\\BigData\\SPARK_2.4\\python\\pyspark\\sql\\utils.py\", line 69, in deco\n    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\npyspark.sql.utils.AnalysisException: \"cannot resolve '`tweet`' given input columns: []; line 1 pos 7;\\n'Project ['tweet]\\n+- SubqueryAlias `temporary_table`\\n   +- LogicalRDD false\\n\"\n\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\r\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\r\n\tat scala.util.Try$.apply(Try.scala:192)\r\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)\r\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-64474ac8612d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\BigData\\SPARK_2.4\\python\\pyspark\\streaming\\context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \"\"\"\n\u001b[0;32m    191\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\BigData\\SPARK_2.4\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\BigData\\SPARK_2.4\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\BigData\\SPARK_2.4\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o214.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"C:\\BigData\\SPARK_2.4\\python\\pyspark\\sql\\utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"C:\\BigData\\SPARK_2.4\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o21.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`tweet`' given input columns: []; line 1 pos 7;\n'Project ['tweet]\n+- SubqueryAlias `temporary_table`\n   +- LogicalRDD false\n\r\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.immutable.List.map(List.scala:296)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:643)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\BigData\\SPARK_2.4\\python\\pyspark\\streaming\\util.py\", line 68, in call\n    r = self.func(t, *rdds)\n  File \"C:\\BigData\\SPARK_2.4\\python\\pyspark\\streaming\\dstream.py\", line 161, in <lambda>\n    func = lambda t, rdd: old_func(rdd)\n  File \"<ipython-input-18-64474ac8612d>\", line 7, in <lambda>\n    message= data.foreachRDD( lambda yourRdd: readMyRddsFromKafkaStream( yourRdd ) )\n  File \"<ipython-input-17-2bf111efdf4d>\", line 5, in readMyRddsFromKafkaStream\n    df = spark.sql( \"\"\"SELECT tweet FROM temporary_table \"\"\" )\n  File \"C:\\BigData\\SPARK_2.4\\python\\pyspark\\sql\\session.py\", line 767, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n  File \"C:\\BigData\\SPARK_2.4\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"C:\\BigData\\SPARK_2.4\\python\\pyspark\\sql\\utils.py\", line 69, in deco\n    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\npyspark.sql.utils.AnalysisException: \"cannot resolve '`tweet`' given input columns: []; line 1 pos 7;\\n'Project ['tweet]\\n+- SubqueryAlias `temporary_table`\\n   +- LogicalRDD false\\n\"\n\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\r\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\r\n\tat scala.util.Try$.apply(Try.scala:192)\r\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)\r\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|International rep...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Should page thous...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Establish recogni...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Very she mouth he...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Necessary probabl...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Building spend ma...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Movie approach en...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Church maybe race...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Break cup sign re...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Experience recogn...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Charge society se...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Staff hope next a...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Along close organ...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Ten by card board...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Player also discu...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Reflect join summ...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Finish want soldi...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Machine kind pers...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Candidate about g...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|If drive note num...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Try positive agai...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Program teach sce...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Ready black Congr...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Focus activity wh...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Fact able a third...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|When reduce early...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Assume age person...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Place member trai...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Institution owner...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Parent off projec...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Past read up addr...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Activity oil seni...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Fall condition co...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|If especially sen...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc = StreamingContext(sc,3)\n",
    "kvs= KafkaUtils.createDirectStream(ssc, topics=['tweet1'],kafkaParams={'metadata.broker.list':'localhost:9092'})\n",
    "\n",
    "\n",
    "\n",
    "data = kvs.map( lambda tuple: tuple[1] )\n",
    "message= data.foreachRDD( lambda yourRdd: readMyRddsFromKafkaStream( yourRdd ) )\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-federal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
